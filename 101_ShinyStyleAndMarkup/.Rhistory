seq(1,10,1)
seq(1,10,1)
ggplot(DF, aes(xnorm, ynorm)) +
geom_point(size=5, shape=20)
+
stat_ellipse(geom="polygon", level=0.95, alpha=0.2)
ggplot(DF, aes(xnorm, ynorm)) +
geom_point(size=5, shape=20) +
stat_ellipse(geom="polygon", level=0.95, alpha=0.2)
ggplot(DF, aes(xnorm, ynorm)) +
geom_point(size=5, shape=20) +
stat_ellipse(geom="polygon", level=0.95, alpha=0.2) +
guides(color=guide_legend("Cluster"),fill=guide_legend("Cluster")) +
scale_x_continuous(breaks = seq(1,10,1)) +
scale_y_continuous(breaks = seq(1,10,1))
ggplot(DF, aes(xnorm, ynorm)) +
geom_point(size=5, shape=20) +
stat_ellipse(geom="polygon", level=0.95, alpha=0.2) +
guides(color=guide_legend("Cluster"),fill=guide_legend("Cluster")) +
scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
olive <- data.frame(olive, stringsasfactors=TRUE)
oliveTree <- train(Area ~ ., method="rpart", data=olive)
set.seed(777)
library(pgmm)
data(olive)
olive = olive[,-1]
olive <- data.frame(olive, stringsasfactors=TRUE)
oliveTree <- train(Area ~ ., method="rpart", data=olive)
?caret
?train
?trainControl
oliveTree <- train(I(factor(Area)) ~ ., method="rpart", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
pred <- predict(oliveTree, newdata)
newdata$Area <- factor(newdata$Area)
pred <- predict(oliveTree, newdata)
str(newdata)
pred <- predict(oliveTree, newdata[,-1])
set.seed(777)
library(pgmm)
data(olive)
olive = olive[,-1]
olive <- data.frame(olive)
oliveTree <- train(I(factor(Area)) ~ ., method="rpart", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
pred <- predict(oliveTree, newdata[,-1])
pred
?tree
set.seed(777)
library(pgmm)
data(olive)
olive = olive[,-1]
olive <- data.frame(olive)
oliveTree <- train(I(factor(Area)) ~ ., method="rpart", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(oliveTree, newdata[,-1])
oliveTree <- train(Area ~ ., method="rpart", data=olive)
oliveTree <- train(Area ~ ., method="lda", data=olive)
olive
head(olive)
oliveTree <- train(factor(Area) ~ ., method="lda", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(oliveTree, newdata[,-1])
oliveTree <- train(factor(Area) ~ ., method="rpart", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(oliveTree, newdata[,-1])
oliveTree <- train(Area ~ ., method="rpart", data=olive[1:100,])
predict(oliveTree, newdata[,-1])
predict(oliveTree, newdata)
set.seed(777)
library(pgmm)
data(olive)
olive = olive[,-1]
olive <- data.frame(olive)
oliveTree <- train(Area ~ ., method="rpart", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(oliveTree, newdata)
library(ElemStatLearn)
data(SAheart)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
colnames(trainSA)
LDA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
LDA <- train(factor(chd) ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
head(trainSA$chd)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- missClass(LDA, testSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- missClass(LDA, testSA)
pred <- missClass(testSA, LDA)
class(testSA)
class(LDA)
LDA[[1]]
LDA[[2]]
LDA[[3]]
LDA[[4]]
LDA[[5]]
pred <- predict(LDA, testSA)
pred <- missClass(testSA$chd, pred)
pred
pred <- predict(LDA, testSA)
pred
LDA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- predict(LDA, testSA)
pred <- missClass(testSA$chd, pred)
pred <- predict(LDA, trainSA)
missClass(testSA$chd, pred)
pred <- predict(LDA, testSA)
missClass(testSA$chd, pred)
pred <- predict(LDA, trainSA)
missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
missClass(testSA$chd, pred)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
set.seed(33833)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = trainDF)
varImp(model)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = trainDF)
varImp(model)
DF <- rbind(trainDF, testDF)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = DF)
varImp(model)
?varImp
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
DF <- rbind(trainDF, testDF)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = testDF)
varImp(model)
model <- train(y ~ ., method = "rf", data = trainDF)
varImp(model)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = DF)
varImp(model)
varImp.RandomForest(model)
varImp.RandomForest(model)
order(varImp(model), decreasing = T)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
caret
?caret
??caret
model <- train(y ~ ., data = trainDF, method = "rf", prox=TRUE)
order(varImp(model), decreasing = T)
varImp(model)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- train(y ~ ., data = trainDF, method="rf")
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
# Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol
# consumption, obesity levels, cumulative tabacco, type-A behavior,
# and low density lipoprotein cholesterol as predictors
LDA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- predict(LDA, trainSA)
missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
missClass(testSA$chd, pred)
pred <- predict(LDA, trainSA)
train <- missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
test <- missClass(testSA$chd, pred)
print("training missclassification", train, "testing missclassification", test)
print("training missclassification", train, "testing missclassification", test)
print(test, train)
pred <- predict(LDA, trainSA)
train <- missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
test <- missClass(testSA$chd, pred)
test
train
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
DF <- rbind(trainDF, testDF)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
library(bdpv)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
str(nPV)
str(result)
result[[1]]
result[[2]]
result[[3]]
result[[4]]
result[[5]]
result[[7]]
result[[6]]
result[[2]]
prev = 0.2 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = 0.13, pi.10 = 0, alpha = 0.1, beta = 0.2, sided = 2)
samsize.mcnemar(pi.01 = 0.11, pi.10 = 0, alpha = 0.1, beta = 0.2, sided = 2)
1-.86
samsize.mcnemar(pi.01 = 0.11, pi.10 = .2, alpha = 0.1, beta = 0.2, sided = 2)
library(caret)
predictors = data.frame(ozone=ozone$ozone)
library(MASS)
predictors = data.frame(ozone=ozone$ozone)
library(datasets)
predictors = data.frame(ozone=ozone$ozone)
??ozone
library(ELSR)
library(elsr)
library(ElemStatLearn)
predictors = data.frame(ozone=ozone$ozone)
?bag
?bagControl
?trcontrol
?trControl
??trcontrol
?qda
??Mass
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
QDAbag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = qda,
predict = predict.qda
aggregate = qda$aggregate)
library(caret)
library(ElemStatLearn)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
QDAbag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = qda,
predict = predict.qda,
aggregate = qda$aggregate))
qda$aggregate
library(caret)
#Question 1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
training <- as.data.frame(data(vowel.train))
testing <- as.data.frame(data(vowel.test))
set.seed(33833)
rforest <- train(factor(y) ~ ., method = "rf", data=training)
colnames(training)
data(vowel.train)
data(vowel.test)
training <- as.data.frame(vowel.train)
testing <- as.data.frame(vowel.test)
set.seed(33833)
rforest <- train(factor(y) ~ ., method = "rf", data=training)
boosting <- train(factor(y) ~ ., method = "gbm", data=training)
class(testing$y)
training <- as.factor(training$y)
testing <- as.factor(testing$y)
set.seed(33833)
rforest <- train(y ~ ., method = "rf", data=training)
boosting <- train(y ~ ., method = "gbm", data=training)
pred <- predict(rforest, testing)
confusionMatrix(testing$y, pred)
rforest <- train(y ~ ., method = "rf", data=training)
data(vowel.train)
data(vowel.test)
training <- as.data.frame(vowel.train)
testing <- as.data.frame(vowel.test)
training$y <- as.factor(training$y)
testing$y <- as.factor(testing$y)
set.seed(33833)
rforest <- train(y ~ ., method = "rf", data=training)
boosting <- train(y ~ ., method = "gbm", data=training)
pred1 <- predict(rforest, testing)
confusionMatrix(testing$y, pred1)
pred2 <- predict(boosting, testing)
confusionMatrix(testing$y, pred2)
predDF <- data.frame(pred1, pred2, y = testing$y)
combModFit <- train(y ~., method="gam", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(predDF$y, combPred)
class(testing$y)
?gam
combModFit <- train(y ~., method="gam", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(predDF$y, combPred)
version
install.packages("devtools")
devtools::install_github('rstudio/shinyapps')
install.packages("rattle")
install.packages("swirl")
install.packages("reshape2")
install.packages("pracma")
install.packages("png")
install.packages("gridExtra")
install.packages("xtable")
library(shinyapps)
devtools::install_github('rstudio/rsconnect')
install.packages("shiny")
install.packages("Rtools")
install.packages("Rtools")
library(shiny)
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/09_DataProducts/DataProductsRepo/FirstShiny")
library(shiny)
runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
