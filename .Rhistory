currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
str(nPV)
str(result)
result[[1]]
result[[2]]
result[[3]]
result[[4]]
result[[5]]
result[[7]]
result[[6]]
result[[2]]
prev = 0.2 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = 0.13, pi.10 = 0, alpha = 0.1, beta = 0.2, sided = 2)
samsize.mcnemar(pi.01 = 0.11, pi.10 = 0, alpha = 0.1, beta = 0.2, sided = 2)
1-.86
samsize.mcnemar(pi.01 = 0.11, pi.10 = .2, alpha = 0.1, beta = 0.2, sided = 2)
library(caret)
predictors = data.frame(ozone=ozone$ozone)
library(MASS)
predictors = data.frame(ozone=ozone$ozone)
library(datasets)
predictors = data.frame(ozone=ozone$ozone)
??ozone
library(ELSR)
library(elsr)
library(ElemStatLearn)
predictors = data.frame(ozone=ozone$ozone)
?bag
?bagControl
?trcontrol
?trControl
??trcontrol
?qda
??Mass
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
QDAbag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = qda,
predict = predict.qda
aggregate = qda$aggregate)
library(caret)
library(ElemStatLearn)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
QDAbag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = qda,
predict = predict.qda,
aggregate = qda$aggregate))
qda$aggregate
library(caret)
#Question 1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
training <- as.data.frame(data(vowel.train))
testing <- as.data.frame(data(vowel.test))
set.seed(33833)
rforest <- train(factor(y) ~ ., method = "rf", data=training)
colnames(training)
data(vowel.train)
data(vowel.test)
training <- as.data.frame(vowel.train)
testing <- as.data.frame(vowel.test)
set.seed(33833)
rforest <- train(factor(y) ~ ., method = "rf", data=training)
boosting <- train(factor(y) ~ ., method = "gbm", data=training)
class(testing$y)
training <- as.factor(training$y)
testing <- as.factor(testing$y)
set.seed(33833)
rforest <- train(y ~ ., method = "rf", data=training)
boosting <- train(y ~ ., method = "gbm", data=training)
pred <- predict(rforest, testing)
confusionMatrix(testing$y, pred)
rforest <- train(y ~ ., method = "rf", data=training)
data(vowel.train)
data(vowel.test)
training <- as.data.frame(vowel.train)
testing <- as.data.frame(vowel.test)
training$y <- as.factor(training$y)
testing$y <- as.factor(testing$y)
set.seed(33833)
rforest <- train(y ~ ., method = "rf", data=training)
boosting <- train(y ~ ., method = "gbm", data=training)
pred1 <- predict(rforest, testing)
confusionMatrix(testing$y, pred1)
pred2 <- predict(boosting, testing)
confusionMatrix(testing$y, pred2)
predDF <- data.frame(pred1, pred2, y = testing$y)
combModFit <- train(y ~., method="gam", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(predDF$y, combPred)
class(testing$y)
?gam
combModFit <- train(y ~., method="gam", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(predDF$y, combPred)
version
install.packages("devtools")
devtools::install_github('rstudio/shinyapps')
install.packages("rattle")
install.packages("swirl")
install.packages("reshape2")
install.packages("pracma")
install.packages("png")
install.packages("gridExtra")
install.packages("xtable")
library(shinyapps)
devtools::install_github('rstudio/rsconnect')
install.packages("shiny")
install.packages("Rtools")
install.packages("Rtools")
data <- matrix(c(6, 2, 8, 4), ncol=2, byrow=T)
mcnemar.test(data)
?mcnemar.test
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = 0.13, pi.10 = 0, alpha = 0.05, beta = 0.2, sided = 2)
data
data[1,1]
data[1,2]
samsize.mcnemar(pi.01 = data[1,2], pi.10 = data[2,1], alpha = 0.05, beta = 0.2, sided = 2)
data[1,2]
data[2,1]
samsize.mcnemar(pi.01 = as.numeric(data[1,2]), pi.10 = as.numeric(data[2,1]), alpha = 0.05, beta = 0.2, sided = 2)
pi.01 = as.numeric(data[1,2])
pi.10 = as.numeric(data[2,1])
data <- matrix(c(75, 15, 1, 4), ncol=2, byrow=T)
data
data <- matrix(c(75, 1, 15, 4), ncol=2, byrow=T)
data
samsize.mcnemar(pi.01 = (data[1,2]/100), pi.10 = (data[2,1]/100), alpha = 0.05, beta = 0.2, sided = 2)
data <- matrix(c(1, 75, 4, 15), ncol=2, byrow=T)
data
data <- matrix(c(15, 4, 75, 1), ncol=2, byrow=T)
data
data <- matrix(c(4, 15, 1, 75), ncol=2, byrow=T)
data
samsize.mcnemar(pi.01 = (data[1,2]/100), pi.10 = (data[2,1]/100), alpha = 0.05, beta = 0.2, sided = 2)
specificityMatrix <- matrix(c(4, 15, 1, 75), ncol=2, byrow=T)
mcnemar.test(data)
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = (specificityMatrix[1,2]/100),
pi.10 = (specificityMatrix[2,1]/100),
alpha = 0.05,
beta = 0.2,
sided = 2)
sensitivityMatrix <- matrix(c(75, 1, 15, 4), ncol=2, byrow=T)
sensitivityMatrix
specificityMatrix <- matrix(c(4, 15, 1, 75), ncol=2, byrow=T)
sensitivityMatrix
specificityMatrix
sum(sensitivityMatrix)
sum(sensitivityMatrix[1,])
sensitivityMatrix <- matrix(c(75, 1, 15, 4), ncol=2, byrow=T)
sensitivityMatrix
specificityMatrix <- matrix(c(4, 15, 1, 75), ncol=2, byrow=T)
specificityMatrix
sum(sensitivityMatrix)
sensitivityMatrix <- matrix(c(76, 1, 19, 4), ncol=2, byrow=T)
sensitivityMatrix
specificityMatrix <- matrix(c(4, 19, 1, 76), ncol=2, byrow=T)
specificityMatrix
mcnemar.test(specificityMatrix)
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = (specificityMatrix[1,2]/100),
pi.10 = (specificityMatrix[2,1]/100),
alpha = 0.05,
beta = 0.2,
sided = 2)
specificityMatrix <- matrix(c(4, 19, 1, 76), ncol=2, byrow=T)
specificityMatrix
mcnemar.test(specificityMatrix)
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = (specificityMatrix[1,2]/sum(specificityMatrix),
pi.10 = (specificityMatrix[2,1]/sum(specificityMatrix)),
alpha = 0.05,
beta = 0.2,
sided = 2)
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = (specificityMatrix[1,2]/sum(specificityMatrix)),
pi.10 = (specificityMatrix[2,1]/sum(specificityMatrix)),
alpha = 0.05,
beta = 0.2,
sided = 2)
### Set WD, Load Libraries and Data
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/08_PracticalMachineLearning/MachineLearningCoursera")
library(caret)
library(ggplot2)
library(moments)
library(stringr)
library(dplyr)
library(tidyr)
source("multiplot.R")
###load training dataset
trainData <- as.data.frame(read.csv("WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv", na.strings=c("","NA")))
###generate ID variable
trainData$windowID <- paste(trainData$user_name, trainData$num_window,
trainData$classe,
sep=" ")
###Create unique identifier column for each user's activities
trainData$exerciseID <- paste(trainData$user_name,
trainData$classe,
sep=" ")
###Graph timeseries
# generate times for each activity
trainData$time <- paste(str_sub(trainData$raw_timestamp_part_1, -5, -1),
substr(trainData$raw_timestamp_part_2, 1, 3),
sep="")
trainData <- trainData %>%
group_by(classe) %>%
group_by(user_name) %>%
mutate(newTime = as.numeric(time) - min(as.numeric(time)))
# plot timeseries for a few data points
p1 <- ggplot(trainData[grep("carlitos A", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="green")
p2 <- ggplot(trainData[grep("carlitos B", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")
p3 <- ggplot(trainData[grep("carlitos C", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")
p4 <- ggplot(trainData[grep("carlitos D", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")
p5 <- ggplot(trainData[grep("carlitos E", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")
multiplot(p1, p2, p3, p4, p5, cols = 2)
###Eliminate columns with missing and unrelated
cleanData <- function(dataframe){
#Eliminate columns with unrelated data
colToRemove <- c(grep("X", colnames(trainData)),
grep("user_name", colnames(trainData)),
grep("raw_timestamp_part_1", colnames(trainData)),
grep("raw_timestamp_part_2", colnames(trainData)),
grep("cvtd_timestamp", colnames(trainData)),
grep("new_window", colnames(trainData)),
grep("num_window", colnames(trainData)),
grep("time", colnames(trainData)),
grep("newTime", colnames(trainData)),
grep("classe", colnames(trainData)))
dataframe <- dataframe[,-colToRemove]
#Eliminate columns with NA values
dataframe <- dataframe[, colSums(is.na(dataframe)) == 0]
return(dataframe)
}
trainData <- cleanData(trainData)
###creat time series covariates/features
featureExtract <- function(dataframe, groupElement){
temp.mean <- aggregate(dataframe, by = list(groupElement), FUN = "mean")
temp.var <- aggregate(dataframe, by = list(groupElement), FUN = "var")
temp.sd <- aggregate(dataframe, by = list(groupElement), FUN = "sd")
temp.max <- aggregate(dataframe, by = list(groupElement), FUN = "max")
temp.min <- aggregate(dataframe, by = list(groupElement), FUN = "min")
temp.amp <- temp.max[,2:52] - temp.min[,2:52]
temp.skew <- aggregate(dataframe, by = list(groupElement), FUN = "skewness")
temp.kurt <- aggregate(dataframe, by = list(groupElement), FUN = "kurtosis")
colnames(temp.mean) <- paste(colnames(temp.mean), "_mean", sep="")
colnames(temp.var) <- paste(colnames(temp.var), "_var", sep="")
colnames(temp.sd) <- paste(colnames(temp.sd), "_sd", sep="")
colnames(temp.max) <- paste(colnames(temp.max), "_max", sep="")
colnames(temp.min) <- paste(colnames(temp.min), "_min", sep="")
colnames(temp.amp) <- paste(colnames(temp.amp), "_amp", sep="")
colnames(temp.skew) <- paste(colnames(temp.skew), "_skew", sep="")
colnames(temp.kurt) <- paste(colnames(temp.kurt), "_kurtosis", sep="")
#join features into DF
temp.features <- as.data.frame(cbind(temp.mean, temp.var, temp.sd,
temp.max, temp.min, temp.amp,
temp.skew, temp.kurt))
#remove columns with NA
temp.features <- temp.features[, colSums(is.na(temp.features)) == 0]
return(temp.features)
}
featuresDF <- featureExtract(trainData[,1:51], trainData$windowID)
# Add `classe` and `user_name` variables back to feature DF
featuresDF$classe <- str_sub(featuresDF$Group.1_kurtosis, -1)
featuresDF$user_name <- str_sub(featuresDF$Group.1_kurtosis, 1, 5)
# remove extra group labels
featuresDF <- featuresDF[,-grep("Group", colnames(featuresDF))]
#Separate training and test data
# We will remove one person from each level of the variable `classe` as the test set
set.seed(1000)
trainIndex <- createDataPartition(featuresDF$classe, p = 0.60,list=FALSE)
training <- featuresDF[trainIndex,]
testing <- featuresDF[-trainIndex,]
###Remove User Name from each dataset
training <- training[,-grep("user_name", colnames(training))]
testing <- testing[,-grep("user_name", colnames(testing))]
### Train lda Classifier
#fit model
Lda <- train(classe ~ ., method="lda", data=training)
#predict test data
pred <- predict(Lda, testing)
confusionMatrix(testing$classe, pred)
library(rattle)
#fit model
tree <- train(classe ~ ., method="rf", data=training)
#predict test data
pred <- predict(tree, testing)
confusionMatrix(testing$classe, pred)
fancyRpartPlot(tree$finalModel)
names(tree)
tree$finalModel
tree$bestTune
trees[[1]]
trees[[2]]
trees[[3]]
trees[[4]]
trees[[5]]
trees[[,1]]
trees[[1,]]
trees[1,
}
trees[1,]
trees[2,]
trees[3,]
trees[4,]
trees[5,]
tree[[1]]
tree[[2]]
tree[[3]]
tree[[4]]
tree[[5]]
tree[[6]]
tree[[7]]
tree[[8]]
tree[[9]]
tree[[20]]
tree[[10]]
tree[[11]]
tree[[12]]
tree[[13]]
tree[[14]]
tree[[15]]
tree[[16]]
tree[[17]]
tree[[18]]
tree[[19]]
tree[[20]]
tree[[21]]
tree[[22]]
tree[[23]]
#fit model
bagging <- train(classe ~ ., method="treebag", data=training)
#predict test data
pred <- predict(bagging, testing)
confusionMatrix(testing$classe, pred)
fancyRpartPlot(summary(bagging)$mtrees[[25]]$btree)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
myPlot
### Quiz 1
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
### Quiz 1
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
### Quiz 1
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
require(devtools)
install_github('rCharts', 'ramnathv')
library(rCharts)
dTable(airquality, sPaginationType = "full_numbers")
data(airquality)
dTable(airquality, sPaginationType = "full_numbers")
?dtable
d <- data.frame(airquality, stringsAsFactors = FALSE)print(d)
d
airquality
head(airquality)
library(rCharts)
dTable(airquality, sPaginationType = "full_numbers")
colnames(airquality)
?dtable
install.packages("base64enc")
require(base64enc)
library(devtools)
require(rCharts)
data("airquality")
table1 <- dTable(airquality, sPaginationType = "full_numbers")
table1
table1$save('./fig/table1.html', standalone=TRUE)
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/09_DataProducts/DataProductsRepo")
table1$save("C:/Users/jeffthatcher/Cloud Drive/RRepos/09_DataProducts/DataProductsRepo", standalone=TRUE)
table1$save("C:/Users/jeffthatcher/Cloud Drive/RRepos/09_DataProducts/DataProductsRepo/fig.html", standalone=TRUE)
library(shiny)
runShinyApp()
shiny::runApp('05_QuizQuestion')
shiny::runApp('05_QuizQuestion')
shiny::runApp('04_renderPlotExample')
shiny::runApp('04_renderPlotExample')
